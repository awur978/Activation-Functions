# Activation-Functions
This is a repository containing the new activation function proposed during the cause of my PhD. 
I will keep updating this repo as time goes on
Language and platforms includes

MATLAB, Python, Tensorflow and Keras
 
 tf_SQNL is the replacement for the popular and computationally expensive tanh activation function
 tf_sqnlsig is the replacement for the popular and computationally expensive Sigmoid activation function
 tf_SQLU is the replacement for the computationally expensive ELU defined in the article " Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)"
 
